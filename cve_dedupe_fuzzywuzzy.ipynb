{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Read CVSS CSV file, clean and sort it, remove all except CVE, Description columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_file = './data_in/CVSSData.csv.gz'\n",
    "output_file = './cleaned_optimized_fuzzy_deduplicated_file.csv.gz'\n",
    "removed_file = './removed_duplicates.csv.gz'\n",
    "exact_file = './exact_duplicates.csv.gz'\n",
    "output_json_file = './duplicate_info.json.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVE-1999-0095</td>\n",
       "      <td>The debug command in Sendmail is enabled, allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVE-1999-0082</td>\n",
       "      <td>CWD ~root command in ftpd allows root access.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVE-1999-1471</td>\n",
       "      <td>Buffer overflow in passwd in BSD based operati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CVE-1999-1122</td>\n",
       "      <td>Vulnerability in restore in SunOS 4.0.3 and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVE-1999-1467</td>\n",
       "      <td>Vulnerability in rcp on SunOS 4.0.x allows rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275716</th>\n",
       "      <td>CVE-2025-0214</td>\n",
       "      <td>A vulnerability was found in TMD Custom Header...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275717</th>\n",
       "      <td>CVE-2024-13130</td>\n",
       "      <td>A vulnerability was found in Dahua IPC-HFW1200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275718</th>\n",
       "      <td>CVE-2024-13131</td>\n",
       "      <td>A vulnerability classified as problematic has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275719</th>\n",
       "      <td>CVE-2024-13132</td>\n",
       "      <td>A vulnerability classified as problematic was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275720</th>\n",
       "      <td>CVE-2024-13133</td>\n",
       "      <td>A vulnerability, which was classified as criti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275721 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   CVE                                        Description\n",
       "0        CVE-1999-0095  The debug command in Sendmail is enabled, allo...\n",
       "1        CVE-1999-0082      CWD ~root command in ftpd allows root access.\n",
       "2        CVE-1999-1471  Buffer overflow in passwd in BSD based operati...\n",
       "3        CVE-1999-1122  Vulnerability in restore in SunOS 4.0.3 and ea...\n",
       "4        CVE-1999-1467  Vulnerability in rcp on SunOS 4.0.x allows rem...\n",
       "...                ...                                                ...\n",
       "275716   CVE-2025-0214  A vulnerability was found in TMD Custom Header...\n",
       "275717  CVE-2024-13130  A vulnerability was found in Dahua IPC-HFW1200...\n",
       "275718  CVE-2024-13131  A vulnerability classified as problematic has ...\n",
       "275719  CVE-2024-13132  A vulnerability classified as problematic was ...\n",
       "275720  CVE-2024-13133  A vulnerability, which was classified as criti...\n",
       "\n",
       "[275721 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(input_file,quoting=csv.QUOTE_ALL, escapechar='\\\\', compression='gzip')\n",
    "df = df[['CVE', 'Description']]\n",
    "#df=df[:50000] #test sample\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_description(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove newlines and carriage returns\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def mark_and_count_dupes(txt_col, threshold=80, window=1000):\n",
    "    txt_list = txt_col.to_list()\n",
    "    marked = [True] * len(txt_list)\n",
    "    duplicate_count = 0\n",
    "    duplicate_groups = []\n",
    "    \n",
    "    for i in tqdm(range(len(txt_list)), desc=\"Checking for duplicates\"):\n",
    "        if not marked[i]:  # don't check duplicates of text rows marked for removal\n",
    "            continue\n",
    "        \n",
    "        group = [i]\n",
    "        # Define the window\n",
    "        start = max(0, i + 1)\n",
    "        end = min(len(txt_list), i + window + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if marked[j]:  # only look through vals not already marked for removal\n",
    "                if fuzz.ratio(txt_list[i], txt_list[j], score_cutoff=threshold):\n",
    "                    marked[j] = False  # mark for removal\n",
    "                    duplicate_count += 1\n",
    "                    group.append(j)\n",
    "        \n",
    "        if len(group) > 1:\n",
    "            duplicate_groups.append(group)\n",
    "    \n",
    "    return marked, duplicate_count, duplicate_groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_duplicate_info(df, duplicate_groups, output_file, include_examples=True):\n",
    "    \"\"\"\n",
    "    Save comprehensive duplicate information to a JSON file with enhanced analytics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing CVE data\n",
    "        duplicate_groups: List of duplicate groups\n",
    "        output_file: Path to save the JSON output\n",
    "        include_examples: Whether to include example matches\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    import json\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    from rapidfuzz import fuzz\n",
    "    \n",
    "    # Sort duplicate_groups by size (largest to smallest)\n",
    "    duplicate_groups.sort(key=len, reverse=True)\n",
    "    \n",
    "    # Prepare detailed analytics\n",
    "    group_sizes = [len(group) for group in duplicate_groups]\n",
    "    size_counter = Counter(group_sizes)\n",
    "    sorted_sizes = sorted(size_counter.items(), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_duplicates = sum(len(group) - 1 for group in duplicate_groups)\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats = {\n",
    "        \"summary\": {\n",
    "            \"total_cves\": len(df),\n",
    "            \"total_duplicate_groups\": len(duplicate_groups),\n",
    "            \"total_duplicate_entries\": total_duplicates,\n",
    "            \"unique_group_sizes\": len(size_counter),\n",
    "            \"average_group_size\": np.mean(group_sizes),\n",
    "            \"median_group_size\": np.median(group_sizes),\n",
    "            \"largest_group_size\": max(group_sizes),\n",
    "            \"deduplication_ratio\": total_duplicates / len(duplicate_groups),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"size_distribution\": [\n",
    "            {\n",
    "                \"group_size\": size,\n",
    "                \"count\": count,\n",
    "                \"percentage\": (count * size / len(df)) * 100\n",
    "            }\n",
    "            for size, count in sorted_sizes\n",
    "        ],\n",
    "        \"percentiles\": {\n",
    "            f\"p{p}\": np.percentile(group_sizes, p)\n",
    "            for p in [25, 50, 75, 90, 95, 99]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Prepare detailed group information\n",
    "    duplicate_info = []\n",
    "    for group in duplicate_groups:\n",
    "        # Get base description for comparison\n",
    "        base_desc = df.iloc[group[0]]['Clean_Description']\n",
    "        \n",
    "        # Calculate similarity scores for all members\n",
    "        similarities = []\n",
    "        for idx in group[1:]:\n",
    "            comp_desc = df.iloc[idx]['Clean_Description']\n",
    "            score = fuzz.ratio(base_desc, comp_desc)\n",
    "            similarities.append(score)\n",
    "        \n",
    "        group_info = {\n",
    "            \"group_size\": len(group),\n",
    "            \"base_cve\": df.iloc[group[0]]['CVE'],\n",
    "            \"base_description\": df.iloc[group[0]]['Description'],\n",
    "            \"similarity_stats\": {\n",
    "                \"min_score\": min(similarities) if similarities else 100,\n",
    "                \"max_score\": max(similarities) if similarities else 100,\n",
    "                \"avg_score\": np.mean(similarities) if similarities else 100\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if include_examples:\n",
    "            # Include a few example matches with their similarity scores\n",
    "            group_info[\"examples\"] = [\n",
    "                {\n",
    "                    \"cve\": df.iloc[idx]['CVE'],\n",
    "                    \"description\": df.iloc[idx]['Description'],\n",
    "                    \"similarity_score\": score\n",
    "                }\n",
    "                for idx, score in zip(group[1:3], similarities[:2])  # First 2 examples\n",
    "            ]\n",
    "        \n",
    "        duplicate_info.append(group_info)\n",
    "    \n",
    "    # Combine all information\n",
    "    result = {\n",
    "        \"metadata\": {\n",
    "            \"analysis_date\": datetime.now().isoformat(),\n",
    "            \"threshold\": 80,  # Add your actual threshold here\n",
    "            \"window_size\": 1000  # Add your actual window size here\n",
    "        },\n",
    "        \"statistics\": stats,\n",
    "        \"groups\": duplicate_info[:100],  # Top 100 largest groups\n",
    "        \"sample_analysis\": {\n",
    "            \"largest_groups\": duplicate_info[:5],  # Top 5 largest groups\n",
    "            \"medium_groups\": duplicate_info[len(duplicate_info)//2:len(duplicate_info)//2+3],  # 3 medium-sized groups\n",
    "            \"threshold_analysis\": {\n",
    "                \"high_similarity\": [g for g in duplicate_info[:10] if g[\"similarity_stats\"][\"min_score\"] > 90],\n",
    "                \"medium_similarity\": [g for g in duplicate_info[:10] if 80 <= g[\"similarity_stats\"][\"min_score\"] <= 90]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to compressed JSON file\n",
    "    with gzip.open(output_file, 'wt', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Enhanced duplicate information saved to {output_file}\")\n",
    "    \n",
    "    # Save a separate file for exact duplicates\n",
    "    exact_dupes = [group for group in duplicate_info \n",
    "                   if group[\"similarity_stats\"][\"min_score\"] == 100]\n",
    "    \n",
    "    exact_output = output_file.replace('.json.gz', '_exact.json.gz')\n",
    "    with gzip.open(exact_output, 'wt', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"metadata\": result[\"metadata\"],\n",
    "            \"statistics\": {\n",
    "                \"total_exact_duplicates\": len(exact_dupes),\n",
    "                \"exact_duplicate_ratio\": len(exact_dupes) / len(duplicate_groups)\n",
    "            },\n",
    "            \"exact_duplicate_groups\": exact_dupes\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Exact duplicate information saved to {exact_output}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duplicate_counts(duplicate_groups, output_file):\n",
    "    \"\"\"\n",
    "    Create and save a horizontal bar plot of duplicate group sizes.\n",
    "    \n",
    "    Args:\n",
    "        duplicate_groups: List of duplicate groups\n",
    "        output_file: Base output file path for determining plot name\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Create images directory if it doesn't exist\n",
    "    images_dir = Path('images')\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Count the sizes of duplicate groups\n",
    "    group_sizes = [len(group) for group in duplicate_groups]\n",
    "    size_counter = Counter(group_sizes)\n",
    "    \n",
    "    # Sort the sizes in descending order\n",
    "    sorted_sizes = sorted(size_counter.items(), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Take top 100 or all if less than 100\n",
    "    top_100 = sorted_sizes[:100]\n",
    "    sizes, counts = zip(*top_100)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 20))  # Increased figure size\n",
    "    bars = plt.barh(range(len(sizes)), counts, align='center')\n",
    "    plt.ylabel('Group Size')\n",
    "    plt.xlabel('Count of Groups')\n",
    "    plt.title('Top 100 Largest Duplicate Group Sizes')\n",
    "    \n",
    "    # Set y-ticks to show group sizes\n",
    "    plt.yticks(range(len(sizes)), sizes)\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, (size, count) in enumerate(zip(sizes, counts)):\n",
    "        plt.text(count, i, f' {count}', va='center')\n",
    "    \n",
    "    # Adjust layout to prevent clipping of labels\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot in images directory\n",
    "    plot_filename = 'duplicate_groups_distribution.png'\n",
    "    plot_path = images_dir / plot_filename\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Top 100 largest duplicate group sizes plot saved to {plot_path}\")\n",
    "    return str(plot_path)  # Return the path for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_duplicate_analysis_plots(duplicate_groups, output_dir='images'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for duplicate analysis with improved readability.\n",
    "    \n",
    "    Args:\n",
    "        duplicate_groups: List of duplicate groups\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    images_dir = Path(output_dir)\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set basic style parameters\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Prepare data\n",
    "    group_sizes = [len(group) for group in duplicate_groups]\n",
    "    size_counter = Counter(group_sizes)\n",
    "    sorted_sizes = sorted(size_counter.items(), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Enhanced duplicate group sizes plot\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "    \n",
    "    # Use scatter plot instead of bar chart to show all points\n",
    "    sizes, counts = zip(*sorted_sizes)\n",
    "    \n",
    "    # Create a colormap based on group size\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(sizes)))\n",
    "    \n",
    "    # Plot scatter with connecting lines for better visualization\n",
    "    ax1.plot(counts, sizes, color='lightgray', alpha=0.5, zorder=1)\n",
    "    scatter = ax1.scatter(counts, sizes, c=sizes, cmap='viridis', \n",
    "                         s=100, alpha=0.7, zorder=2)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Group Size', fontsize=12)\n",
    "    \n",
    "    # Improve axes\n",
    "    ax1.set_xlabel('Number of Groups', fontsize=12)\n",
    "    ax1.set_ylabel('Group Size', fontsize=12)\n",
    "    ax1.set_title('Duplicate Group Size Distribution', pad=20, fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Set log scale for better visualization of size distribution\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Add annotations for interesting points\n",
    "    max_count = max(counts)\n",
    "    max_size = max(sizes)\n",
    "    ax1.annotate(f'Largest group: {max_size}',\n",
    "                xy=(size_counter[max_size], max_size),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Distribution plot with log scale\n",
    "    ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "    \n",
    "    # Create histogram with log scale\n",
    "    hist_data = np.array(group_sizes)\n",
    "    n, bins, patches = ax2.hist(hist_data, bins=50, color='purple', alpha=0.7)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_title('Distribution of Duplicate Group Sizes (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Group Size', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency (Log Scale)', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--', which='both')\n",
    "    \n",
    "    # Add statistical annotations\n",
    "    stats_text = f'Mean: {np.mean(hist_data):.2f}\\n'\n",
    "    stats_text += f'Median: {np.median(hist_data):.2f}\\n'\n",
    "    stats_text += f'Mode: {stats.mode(hist_data)[0]:.2f}\\n'\n",
    "    stats_text += f'StdDev: {np.std(hist_data):.2f}\\n'\n",
    "    stats_text += f'Total Groups: {len(hist_data):,}'\n",
    "    ax2.text(0.95, 0.95, stats_text, transform=ax2.transAxes,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'),\n",
    "             fontsize=10)\n",
    "    \n",
    "    # 3. Cumulative distribution plot\n",
    "    ax3 = plt.subplot2grid((2, 2), (1, 1))\n",
    "    \n",
    "    total_duplicates = sum(count * (size - 1) for size, count in size_counter.items())\n",
    "    cumsum = 0\n",
    "    cum_percentages = []\n",
    "    sizes_cum = []\n",
    "    \n",
    "    for size, count in sorted_sizes:\n",
    "        cumsum += count * (size - 1)\n",
    "        cum_percentages.append((cumsum / total_duplicates) * 100)\n",
    "        sizes_cum.append(size)\n",
    "    \n",
    "    # Plot with enhanced styling\n",
    "    ax3.plot(sizes_cum, cum_percentages, color='gold', linewidth=2.5)\n",
    "    ax3.set_title('Cumulative % of Duplicates by Group Size', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Group Size', fontsize=12)\n",
    "    ax3.set_ylabel('Cumulative % of Total Duplicates', fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add percentage markers\n",
    "    percentiles = [25, 50, 75, 90]\n",
    "    for p in percentiles:\n",
    "        idx = next(i for i, x in enumerate(cum_percentages) if x >= p)\n",
    "        ax3.axhline(y=p, color='red', linestyle='--', alpha=0.3)\n",
    "        ax3.text(sizes_cum[idx], p + 1, f'{p}%', va='bottom', ha='right',\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', pad=2),\n",
    "                fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = f'Total Duplicate Groups: {len(duplicate_groups):,}\\n'\n",
    "    summary_stats += f'Total Duplicate Entries: {total_duplicates:,}\\n'\n",
    "    summary_stats += f'Unique Group Sizes: {len(size_counter):,}\\n'\n",
    "    summary_stats += f'Deduplication Ratio: {total_duplicates/len(duplicate_groups):.2f} duplicates/group'\n",
    "    \n",
    "    fig.text(0.02, 0.02, summary_stats, fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'),\n",
    "             fontweight='bold')\n",
    "    \n",
    "    # Add a title for the entire figure\n",
    "    fig.suptitle('CVE Description Duplicate Analysis', fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = images_dir / 'duplicate_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Enhanced duplicate analysis plots saved to {plot_path}\")\n",
    "    return str(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_separate_duplicate_analysis(df, duplicate_groups, output_dir='images'):\n",
    "    \"\"\"\n",
    "    Create separate visualizations for exact and fuzzy duplicates.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with CVE data\n",
    "        duplicate_groups: List of duplicate groups\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from scipy import stats\n",
    "    from rapidfuzz import fuzz\n",
    "    \n",
    "    # Create output directory\n",
    "    images_dir = Path(output_dir)\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Separate exact and fuzzy duplicates\n",
    "    exact_groups = []\n",
    "    fuzzy_groups = []\n",
    "    \n",
    "    for group in duplicate_groups:\n",
    "        base_desc = df.iloc[group[0]]['Clean_Description']\n",
    "        is_exact = True\n",
    "        \n",
    "        # Check if all descriptions in group are exact matches\n",
    "        for idx in group[1:]:\n",
    "            comp_desc = df.iloc[idx]['Clean_Description']\n",
    "            if base_desc != comp_desc:\n",
    "                is_exact = False\n",
    "                break\n",
    "        \n",
    "        if is_exact:\n",
    "            exact_groups.append(group)\n",
    "        else:\n",
    "            fuzzy_groups.append(group)\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # 1. Side-by-side group size distributions (top row)\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "    ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "    \n",
    "    def plot_size_distribution(ax, groups, title, color):\n",
    "        group_sizes = [len(group) for group in groups]\n",
    "        size_counter = Counter(group_sizes)\n",
    "        sorted_sizes = sorted(size_counter.items())\n",
    "        \n",
    "        if sorted_sizes:\n",
    "            sizes, counts = zip(*sorted_sizes)\n",
    "            ax.scatter(sizes, counts, alpha=0.6, c=color, s=100)\n",
    "            ax.plot(sizes, counts, alpha=0.3, c=color)\n",
    "            \n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xscale('log')\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Group Size', fontsize=12)\n",
    "        ax.set_ylabel('Frequency (Log Scale)', fontsize=12)\n",
    "        \n",
    "        # Add statistics\n",
    "        if groups:\n",
    "            all_sizes = [len(g) for g in groups]\n",
    "            stats_text = f'Total Groups: {len(groups):,}\\n'\n",
    "            stats_text += f'Mean Size: {np.mean(all_sizes):.1f}\\n'\n",
    "            stats_text += f'Median Size: {np.median(all_sizes):.1f}\\n'\n",
    "            stats_text += f'Max Size: {max(all_sizes)}'\n",
    "            \n",
    "            ax.text(0.95, 0.95, stats_text,\n",
    "                   transform=ax.transAxes,\n",
    "                   verticalalignment='top',\n",
    "                   horizontalalignment='right',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.9),\n",
    "                   fontsize=10)\n",
    "    \n",
    "    plot_size_distribution(ax1, exact_groups, 'Exact Duplicate Groups', 'blue')\n",
    "    plot_size_distribution(ax2, fuzzy_groups, 'Fuzzy Duplicate Groups', 'orange')\n",
    "    \n",
    "    # 2. Combined cumulative distribution (bottom left)\n",
    "    ax3 = plt.subplot2grid((2, 2), (1, 0))\n",
    "    \n",
    "    def plot_cumulative(groups, label, color):\n",
    "        if not groups:\n",
    "            return\n",
    "        \n",
    "        group_sizes = [len(group) for group in groups]\n",
    "        size_counter = Counter(group_sizes)\n",
    "        sorted_sizes = sorted(size_counter.items())\n",
    "        \n",
    "        total = sum(count for _, count in sorted_sizes)\n",
    "        cumsum = 0\n",
    "        cum_percentages = []\n",
    "        sizes = []\n",
    "        \n",
    "        for size, count in sorted_sizes:\n",
    "            cumsum += count\n",
    "            cum_percentages.append((cumsum / total) * 100)\n",
    "            sizes.append(size)\n",
    "        \n",
    "        ax3.plot(sizes, cum_percentages, label=label, color=color, linewidth=2)\n",
    "    \n",
    "    plot_cumulative(exact_groups, 'Exact Duplicates', 'blue')\n",
    "    plot_cumulative(fuzzy_groups, 'Fuzzy Duplicates', 'orange')\n",
    "    \n",
    "    ax3.set_xscale('log')\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax3.set_title('Cumulative Distribution of Group Sizes', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Group Size', fontsize=12)\n",
    "    ax3.set_ylabel('Cumulative %', fontsize=12)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 3. Summary statistics (bottom right)\n",
    "    ax4 = plt.subplot2grid((2, 2), (1, 1))\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    total_exact = sum(len(group) - 1 for group in exact_groups)\n",
    "    total_fuzzy = sum(len(group) - 1 for group in fuzzy_groups)\n",
    "    \n",
    "    summary_text = [\n",
    "        \"Duplicate Analysis Summary\",\n",
    "        \"------------------------\",\n",
    "        f\"Total Duplicate Groups: {len(duplicate_groups):,}\",\n",
    "        f\"Exact Duplicate Groups: {len(exact_groups):,}\",\n",
    "        f\"Fuzzy Duplicate Groups: {len(fuzzy_groups):,}\",\n",
    "        \"\",\n",
    "        f\"Total Duplicate Entries: {total_exact + total_fuzzy:,}\",\n",
    "        f\"Exact Duplicates: {total_exact:,}\",\n",
    "        f\"Fuzzy Duplicates: {total_fuzzy:,}\",\n",
    "        \"\",\n",
    "        \"Ratios:\",\n",
    "        f\"Exact/Total Groups: {len(exact_groups)/len(duplicate_groups):.1%}\",\n",
    "        f\"Fuzzy/Total Groups: {len(fuzzy_groups)/len(duplicate_groups):.1%}\",\n",
    "        \"\",\n",
    "        \"Average Group Sizes:\",\n",
    "        f\"Exact Groups: {np.mean([len(g) for g in exact_groups]):.1f}\",\n",
    "        f\"Fuzzy Groups: {np.mean([len(g) for g in fuzzy_groups]):.1f}\"\n",
    "    ]\n",
    "    \n",
    "    ax4.text(0.05, 0.95, '\\n'.join(summary_text),\n",
    "             transform=ax4.transAxes,\n",
    "             verticalalignment='top',\n",
    "             fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9),\n",
    "             fontsize=10)\n",
    "    \n",
    "    # Overall title\n",
    "    plt.suptitle('Exact vs Fuzzy Duplicates Analysis', \n",
    "                fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = images_dir / 'duplicate_comparison.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Duplicate comparison plot saved to {plot_path}\")\n",
    "    return str(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing exact duplicates: (251400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Clean the Description column\n",
    "df['Clean_Description'] = df['Description'].apply(clean_description)\n",
    "\n",
    "# Remove exact duplicates first, using the cleaned description\n",
    "#df = df.drop_duplicates(subset='Clean_Description', keep='first')\n",
    "#print(f\"Shape after removing exact duplicates: {df.shape}\")\n",
    "\n",
    "# Sort the DataFrame by the cleaned description\n",
    "df = df.sort_values('Clean_Description')\n",
    "\n",
    "# Reset index for proper functioning of the fuzzy_dedupe function\n",
    "df = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting deduplication process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking for duplicates:   1%|          | 1399/251400 [00:01<04:26, 937.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting deduplication process...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m chk, dup_count, dup_groups \u001b[38;5;241m=\u001b[39m \u001b[43mmark_and_count_dupes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClean_Description\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dfx \u001b[38;5;241m=\u001b[39m df[chk]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeduplication complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[60], line 37\u001b[0m, in \u001b[0;36mmark_and_count_dupes\u001b[0;34m(txt_col, threshold, window)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, end):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m marked[j]:  \u001b[38;5;66;03m# only look through vals not already marked for removal\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfuzz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxt_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_cutoff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m             marked[j] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# mark for removal\u001b[39;00m\n\u001b[1;32m     39\u001b[0m             duplicate_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting deduplication process...\")\n",
    "chk, dup_count, dup_groups = mark_and_count_dupes(df['Clean_Description'], threshold=80, window=1000)\n",
    "\n",
    "dfx = df[chk]\n",
    "print(f\"Deduplication complete.\")\n",
    "print(f\"Original row count: {len(df)}\")\n",
    "print(f\"Rows remaining after deduplication: {len(dfx)}\")\n",
    "print(f\"Number of duplicates found: {dup_count}\")\n",
    "print(f\"Number of duplicate groups: {len(dup_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 largest duplicate group sizes plot saved to images/duplicate_groups_distribution.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'images/duplicate_groups_distribution.png'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_duplicate_counts(dup_groups, output_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced duplicate analysis plots saved to images/duplicate_analysis.png\n"
     ]
    }
   ],
   "source": [
    "plot_path = create_duplicate_analysis_plots(dup_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/work/CyberSecAI/cve_dedup/dedup/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/chris/work/CyberSecAI/cve_dedup/dedup/lib/python3.12/site-packages/numpy/_core/_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate comparison plot saved to images/duplicate_comparison.png\n"
     ]
    }
   ],
   "source": [
    "plot_path = create_separate_duplicate_analysis(df, dup_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 87642\n",
      "Enhanced duplicate information saved to ./duplicate_info.json.gz\n",
      "Exact duplicate information saved to ./duplicate_info_exact.json.gz\n",
      "Duplicate information saved to ./duplicate_info.json.gz\n"
     ]
    }
   ],
   "source": [
    "# see the removed duplicates:\n",
    "duplicates = df[~pd.Series(chk)]\n",
    "print(f\"Number of duplicates removed: {len(duplicates)}\")\n",
    "# Save the duplicates\n",
    "duplicates.to_csv(removed_file, quoting=csv.QUOTE_ALL, escapechar='\\\\', compression='gzip')\n",
    "\n",
    "\n",
    "# Optionally, save the deduplicated DataFrame\n",
    "dfx.to_csv(output_file, quoting=csv.QUOTE_ALL, escapechar='\\\\', compression='gzip')\n",
    "\n",
    "\n",
    "\n",
    "# Save duplicate information to file\n",
    "save_duplicate_info(df, dup_groups, output_json_file)\n",
    "print(f\"Duplicate information saved to {output_json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
