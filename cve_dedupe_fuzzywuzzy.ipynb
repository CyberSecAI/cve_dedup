{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Read CVSS CSV file, clean and sort it, remove all except CVE, Description columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output file paths\n",
    "input_file = './data_in/CVSSData.csv.gz'\n",
    "output_file = './cleaned_optimized_fuzzy_deduplicated_file.csv.gz'\n",
    "removed_file = './removed_duplicates.csv.gz'\n",
    "exact_file = './exact_duplicates.csv.gz'\n",
    "output_json_file = './duplicate_info.json.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVE-1999-0095</td>\n",
       "      <td>The debug command in Sendmail is enabled, allo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVE-1999-0082</td>\n",
       "      <td>CWD ~root command in ftpd allows root access.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVE-1999-1471</td>\n",
       "      <td>Buffer overflow in passwd in BSD based operati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CVE-1999-1122</td>\n",
       "      <td>Vulnerability in restore in SunOS 4.0.3 and ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CVE-1999-1467</td>\n",
       "      <td>Vulnerability in rcp on SunOS 4.0.x allows rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275716</th>\n",
       "      <td>CVE-2025-0214</td>\n",
       "      <td>A vulnerability was found in TMD Custom Header...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275717</th>\n",
       "      <td>CVE-2024-13130</td>\n",
       "      <td>A vulnerability was found in Dahua IPC-HFW1200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275718</th>\n",
       "      <td>CVE-2024-13131</td>\n",
       "      <td>A vulnerability classified as problematic has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275719</th>\n",
       "      <td>CVE-2024-13132</td>\n",
       "      <td>A vulnerability classified as problematic was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275720</th>\n",
       "      <td>CVE-2024-13133</td>\n",
       "      <td>A vulnerability, which was classified as criti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275721 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   CVE                                        Description\n",
       "0        CVE-1999-0095  The debug command in Sendmail is enabled, allo...\n",
       "1        CVE-1999-0082      CWD ~root command in ftpd allows root access.\n",
       "2        CVE-1999-1471  Buffer overflow in passwd in BSD based operati...\n",
       "3        CVE-1999-1122  Vulnerability in restore in SunOS 4.0.3 and ea...\n",
       "4        CVE-1999-1467  Vulnerability in rcp on SunOS 4.0.x allows rem...\n",
       "...                ...                                                ...\n",
       "275716   CVE-2025-0214  A vulnerability was found in TMD Custom Header...\n",
       "275717  CVE-2024-13130  A vulnerability was found in Dahua IPC-HFW1200...\n",
       "275718  CVE-2024-13131  A vulnerability classified as problematic has ...\n",
       "275719  CVE-2024-13132  A vulnerability classified as problematic was ...\n",
       "275720  CVE-2024-13133  A vulnerability, which was classified as criti...\n",
       "\n",
       "[275721 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(input_file,quoting=csv.QUOTE_ALL, escapechar='\\\\', compression='gzip')\n",
    "df = df[['CVE', 'Description']]\n",
    "#df=df[:50000] #test sample\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_description(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Normalize unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove newlines and carriage returns\n",
    "    text = text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def mark_and_count_dupes(txt_col, threshold=80, window=1000):\n",
    "    txt_list = txt_col.to_list()\n",
    "    marked = [True] * len(txt_list)\n",
    "    duplicate_count = 0\n",
    "    duplicate_groups = []\n",
    "    \n",
    "    for i in tqdm(range(len(txt_list)), desc=\"Checking for duplicates\"):\n",
    "        if not marked[i]:  # don't check duplicates of text rows marked for removal\n",
    "            continue\n",
    "        \n",
    "        group = [i]\n",
    "        # Define the window\n",
    "        start = max(0, i + 1)\n",
    "        end = min(len(txt_list), i + window + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if marked[j]:  # only look through vals not already marked for removal\n",
    "                if fuzz.ratio(txt_list[i], txt_list[j], score_cutoff=threshold):\n",
    "                    marked[j] = False  # mark for removal\n",
    "                    duplicate_count += 1\n",
    "                    group.append(j)\n",
    "        \n",
    "        if len(group) > 1:\n",
    "            duplicate_groups.append(group)\n",
    "    \n",
    "    return marked, duplicate_count, duplicate_groups\n",
    "\n",
    "\n",
    "def save_duplicate_info(df, duplicate_groups, output_file):\n",
    "    # Sort duplicate_groups by size (largest to smallest)\n",
    "    duplicate_groups.sort(key=len, reverse=True)\n",
    "    \n",
    "    duplicate_info = []\n",
    "    for group in duplicate_groups:\n",
    "        group_info = {\n",
    "            \"group_size\": len(group),\n",
    "            \"items\": [\n",
    "                {\n",
    "                    \"index\": idx,\n",
    "                    \"cve\": df.iloc[idx]['CVE'],\n",
    "                    \"description\": df.iloc[idx]['Description'][:200]  # First 200 characters\n",
    "                } for idx in group\n",
    "            ]\n",
    "        }\n",
    "        duplicate_info.append(group_info)\n",
    "    \n",
    "    # Create a list of group sizes\n",
    "    group_sizes = [len(group) for group in duplicate_groups]\n",
    "    size_counter = Counter(group_sizes)\n",
    "    sorted_sizes = sorted(size_counter.items(), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    result = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"total_duplicates\": sum(len(group) - 1 for group in duplicate_groups),\n",
    "        \"duplicate_groups\": len(duplicate_groups),\n",
    "        \"group_size_distribution\": [{\"size\": size, \"count\": count} for size, count in sorted_sizes],\n",
    "        \"groups\": duplicate_info\n",
    "    }\n",
    "\n",
    "    with gzip.open(output_file, 'wt', encoding='utf-8') as f:\n",
    "        json.dump(result, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_duplicate_counts(duplicate_groups, output_file):\n",
    "    \"\"\"\n",
    "    Create and save a horizontal bar plot of duplicate group sizes.\n",
    "    \n",
    "    Args:\n",
    "        duplicate_groups: List of duplicate groups\n",
    "        output_file: Base output file path for determining plot name\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import matplotlib.pyplot as plt\n",
    "    from collections import Counter\n",
    "    \n",
    "    # Create images directory if it doesn't exist\n",
    "    images_dir = Path('images')\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Count the sizes of duplicate groups\n",
    "    group_sizes = [len(group) for group in duplicate_groups]\n",
    "    size_counter = Counter(group_sizes)\n",
    "    \n",
    "    # Sort the sizes in descending order\n",
    "    sorted_sizes = sorted(size_counter.items(), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Take top 100 or all if less than 100\n",
    "    top_100 = sorted_sizes[:100]\n",
    "    sizes, counts = zip(*top_100)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(14, 20))  # Increased figure size\n",
    "    bars = plt.barh(range(len(sizes)), counts, align='center')\n",
    "    plt.ylabel('Group Size')\n",
    "    plt.xlabel('Count of Groups')\n",
    "    plt.title('Top 100 Largest Duplicate Group Sizes')\n",
    "    \n",
    "    # Set y-ticks to show group sizes\n",
    "    plt.yticks(range(len(sizes)), sizes)\n",
    "    \n",
    "    # Add value labels on the bars\n",
    "    for i, (size, count) in enumerate(zip(sizes, counts)):\n",
    "        plt.text(count, i, f' {count}', va='center')\n",
    "    \n",
    "    # Adjust layout to prevent clipping of labels\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot in images directory\n",
    "    plot_filename = 'duplicate_groups_distribution.png'\n",
    "    plot_path = images_dir / plot_filename\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Top 100 largest duplicate group sizes plot saved to {plot_path}\")\n",
    "    return str(plot_path)  # Return the path for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_duplicate_analysis_plots(duplicate_groups, output_dir='images'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for duplicate analysis with improved readability.\n",
    "    \n",
    "    Args:\n",
    "        duplicate_groups: List of duplicate groups\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    images_dir = Path(output_dir)\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Set basic style parameters\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Prepare data\n",
    "    group_sizes = [len(group) for group in duplicate_groups]\n",
    "    size_counter = Counter(group_sizes)\n",
    "    sorted_sizes = sorted(size_counter.items(), key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Enhanced duplicate group sizes plot\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=2)\n",
    "    \n",
    "    # Use scatter plot instead of bar chart to show all points\n",
    "    sizes, counts = zip(*sorted_sizes)\n",
    "    \n",
    "    # Create a colormap based on group size\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(sizes)))\n",
    "    \n",
    "    # Plot scatter with connecting lines for better visualization\n",
    "    ax1.plot(counts, sizes, color='lightgray', alpha=0.5, zorder=1)\n",
    "    scatter = ax1.scatter(counts, sizes, c=sizes, cmap='viridis', \n",
    "                         s=100, alpha=0.7, zorder=2)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Group Size', fontsize=12)\n",
    "    \n",
    "    # Improve axes\n",
    "    ax1.set_xlabel('Number of Groups', fontsize=12)\n",
    "    ax1.set_ylabel('Group Size', fontsize=12)\n",
    "    ax1.set_title('Duplicate Group Size Distribution', pad=20, fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Set log scale for better visualization of size distribution\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Add annotations for interesting points\n",
    "    max_count = max(counts)\n",
    "    max_size = max(sizes)\n",
    "    ax1.annotate(f'Largest group: {max_size}',\n",
    "                xy=(size_counter[max_size], max_size),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "    \n",
    "    # 2. Distribution plot with log scale\n",
    "    ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "    \n",
    "    # Create histogram with log scale\n",
    "    hist_data = np.array(group_sizes)\n",
    "    n, bins, patches = ax2.hist(hist_data, bins=50, color='purple', alpha=0.7)\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_title('Distribution of Duplicate Group Sizes (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Group Size', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency (Log Scale)', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3, linestyle='--', which='both')\n",
    "    \n",
    "    # Add statistical annotations\n",
    "    stats_text = f'Mean: {np.mean(hist_data):.2f}\\n'\n",
    "    stats_text += f'Median: {np.median(hist_data):.2f}\\n'\n",
    "    stats_text += f'Mode: {stats.mode(hist_data)[0]:.2f}\\n'\n",
    "    stats_text += f'StdDev: {np.std(hist_data):.2f}\\n'\n",
    "    stats_text += f'Total Groups: {len(hist_data):,}'\n",
    "    ax2.text(0.95, 0.95, stats_text, transform=ax2.transAxes,\n",
    "             verticalalignment='top', horizontalalignment='right',\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'),\n",
    "             fontsize=10)\n",
    "    \n",
    "    # 3. Cumulative distribution plot\n",
    "    ax3 = plt.subplot2grid((2, 2), (1, 1))\n",
    "    \n",
    "    total_duplicates = sum(count * (size - 1) for size, count in size_counter.items())\n",
    "    cumsum = 0\n",
    "    cum_percentages = []\n",
    "    sizes_cum = []\n",
    "    \n",
    "    for size, count in sorted_sizes:\n",
    "        cumsum += count * (size - 1)\n",
    "        cum_percentages.append((cumsum / total_duplicates) * 100)\n",
    "        sizes_cum.append(size)\n",
    "    \n",
    "    # Plot with enhanced styling\n",
    "    ax3.plot(sizes_cum, cum_percentages, color='gold', linewidth=2.5)\n",
    "    ax3.set_title('Cumulative % of Duplicates by Group Size', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Group Size', fontsize=12)\n",
    "    ax3.set_ylabel('Cumulative % of Total Duplicates', fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add percentage markers\n",
    "    percentiles = [25, 50, 75, 90]\n",
    "    for p in percentiles:\n",
    "        idx = next(i for i, x in enumerate(cum_percentages) if x >= p)\n",
    "        ax3.axhline(y=p, color='red', linestyle='--', alpha=0.3)\n",
    "        ax3.text(sizes_cum[idx], p + 1, f'{p}%', va='bottom', ha='right',\n",
    "                bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray', pad=2),\n",
    "                fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = f'Total Duplicate Groups: {len(duplicate_groups):,}\\n'\n",
    "    summary_stats += f'Total Duplicate Entries: {total_duplicates:,}\\n'\n",
    "    summary_stats += f'Unique Group Sizes: {len(size_counter):,}\\n'\n",
    "    summary_stats += f'Deduplication Ratio: {total_duplicates/len(duplicate_groups):.2f} duplicates/group'\n",
    "    \n",
    "    fig.text(0.02, 0.02, summary_stats, fontsize=11,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='gray'),\n",
    "             fontweight='bold')\n",
    "    \n",
    "    # Add a title for the entire figure\n",
    "    fig.suptitle('CVE Description Duplicate Analysis', fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plot_path = images_dir / 'duplicate_analysis.png'\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Enhanced duplicate analysis plots saved to {plot_path}\")\n",
    "    return str(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing exact duplicates: (251400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Clean the Description column\n",
    "df['Clean_Description'] = df['Description'].apply(clean_description)\n",
    "\n",
    "# Remove exact duplicates first, using the cleaned description\n",
    "df = df.drop_duplicates(subset='Clean_Description', keep='first')\n",
    "print(f\"Shape after removing exact duplicates: {df.shape}\")\n",
    "\n",
    "# Sort the DataFrame by the cleaned description\n",
    "df = df.sort_values('Clean_Description')\n",
    "\n",
    "# Reset index for proper functioning of the fuzzy_dedupe function\n",
    "df = df.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting deduplication process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking for duplicates: 100%|██████████| 251400/251400 [04:11<00:00, 998.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete.\n",
      "Original row count: 251400\n",
      "Rows remaining after deduplication: 163758\n",
      "Number of duplicates found: 87642\n",
      "Number of duplicate groups: 22522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting deduplication process...\")\n",
    "chk, dup_count, dup_groups = mark_and_count_dupes(df['Clean_Description'], threshold=80, window=1000)\n",
    "\n",
    "dfx = df[chk]\n",
    "print(f\"Deduplication complete.\")\n",
    "print(f\"Original row count: {len(df)}\")\n",
    "print(f\"Rows remaining after deduplication: {len(dfx)}\")\n",
    "print(f\"Number of duplicates found: {dup_count}\")\n",
    "print(f\"Number of duplicate groups: {len(dup_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 largest duplicate group sizes plot saved to images/duplicate_groups_distribution.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'images/duplicate_groups_distribution.png'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_duplicate_counts(dup_groups, output_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced duplicate analysis plots saved to images/duplicate_analysis.png\n"
     ]
    }
   ],
   "source": [
    "plot_path = create_duplicate_analysis_plots(dup_groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 87642\n",
      "Duplicate information saved to ./duplicate_info.json.gz\n"
     ]
    }
   ],
   "source": [
    "# see the removed duplicates:\n",
    "duplicates = df[~pd.Series(chk)]\n",
    "print(f\"Number of duplicates removed: {len(duplicates)}\")\n",
    "# Save the duplicates\n",
    "duplicates.to_csv(removed_file, quoting=csv.QUOTE_ALL, escapechar='\\\\', compression='gzip')\n",
    "\n",
    "\n",
    "# Optionally, save the deduplicated DataFrame\n",
    "dfx.to_csv(output_file, quoting=csv.QUOTE_ALL, escapechar='\\\\', compression='gzip')\n",
    "\n",
    "\n",
    "\n",
    "# Save duplicate information to file\n",
    "save_duplicate_info(df, dup_groups, output_json_file)\n",
    "print(f\"Duplicate information saved to {output_json_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
